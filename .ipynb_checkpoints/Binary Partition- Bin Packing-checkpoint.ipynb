{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pprint\n",
    "import operator\n",
    "import random\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Info of Placement\n",
    "vm_external_ip = \"34.141.8.255\" #External ip for host machine to fetch the data\n",
    "kiali_port = 32002\n",
    "prometheus_port = 32003\n",
    "\n",
    "namespace = \"default\" # the namespace of the app \n",
    "\n",
    "cluster_id = \"onlineboutique\" # Cluster name\n",
    "\n",
    "cluster_pool = \"default-pool\" # Node pool\n",
    "\n",
    "project_id = \"single-verve-297917\" # Project-ID\n",
    "\n",
    "zone = \"europe-west3-b\" # Project-zone\n",
    "\n",
    "vm_threshold_per_pod = 0.1 # Threshold for reserving sufficient resources for each pod\n",
    "\n",
    "# Connect to cluster command\n",
    "connection_command = \"gcloud container clusters get-credentials onlineboutique --zone europe-west3-b --project single-verve-297917\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hosts : 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-rj5g': '7224029184.0',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-c73w': '6975311872.0',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '7538466816.0',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': '6998241280.0'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information metrics from prometheus about current nodes and pods\n",
    "\n",
    "# Url from prometheus\n",
    "url_prometheus = \"http://\"+vm_external_ip+\":\"+str(prometheus_port)+\"/api/v1/query\"\n",
    "\n",
    "# RAM USAGE PERCENT\n",
    "# (1 - (node_memory_MemAvailable_bytes / (node_memory_MemTotal_bytes)))* 100\n",
    "# # CPU USAGE PERCENT\n",
    "# (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[30m])) by (instance)) * 100\n",
    "\n",
    "# #PODS CPU USAGE PERCENT (EXCEPT NODE-EXPORTERS)\n",
    "# avg(rate(container_cpu_usage_seconds_total{pod!~\"billowing.*\", namespace='default'}[30m])) by (pod) *100\n",
    "\n",
    "# #PODS MEMORY USAGE (EXCEPT NODE-EXPORTERS)\n",
    "# avg(container_memory_max_usage_bytes{namespace=\"default\", pod!~\"billowing.*\"}) by(pod)\n",
    "\n",
    "# Queries for useful information of Prometheus\n",
    "query_node_cpu = {\"query\":\"avg(rate(node_cpu_seconds_total{mode='idle'}[30m])) by (instance)\"}\n",
    "query_node_ram = {\"query\":\"node_memory_MemAvailable_bytes\"}\n",
    "query_node_totalRam = {\"query\":\"node_memory_MemTotal_bytes\"}\n",
    "\n",
    "# Headers of cURL command\n",
    "headers_prometheus = {\n",
    "    'cache-control': \"no-cache\"\n",
    "}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=query_node_ram)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "number_of_hosts = len(result[\"data\"][\"result\"])\n",
    "host_machines = []\n",
    "node_available_ram = {}\n",
    "print(\"Number of hosts : \" + str(number_of_hosts))\n",
    "for i in range(number_of_hosts):\n",
    "    host_machines.append(result[\"data\"][\"result\"][i][\"metric\"][\"kubernetes_node\"])\n",
    "    node_available_ram[host_machines[i]] = format(float(result[\"data\"][\"result\"][i][\"value\"][1]), '.1f')\n",
    "node_available_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835242393.6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cURL command for Node Ram Total\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=query_node_totalRam)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_total_ram = {}\n",
    "for i in range(number_of_hosts):\n",
    "    node_total_ram[host_machines[i]] = format(float(result[\"data\"][\"result\"][i][\"value\"][1]), '.1f')\n",
    "max_ram_per_pod = float(max(node_total_ram.values())) * vm_threshold_per_pod \n",
    "max_ram_per_pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-rj5g': '0.8554',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-c73w': '0.7614',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '0.9522',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': '0.9130'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cURL command for Node Cpu Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=query_node_cpu)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_available_cpu = {}\n",
    "for i in range(number_of_hosts):\n",
    "     node_available_cpu[host_machines[i]] = format(float(result[\"data\"][\"result\"][i][\"value\"][1]), '.4f')\n",
    "node_available_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-c73w': '1.001',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-rj5g': '1.041',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': '0.793',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '0.963'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"sum(kube_pod_container_resource_requests_cpu_cores) by (node)\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_request_cpu = {}\n",
    "for x in result['data']['result']:\n",
    "    node_request_cpu[x['metric']['node']] = format(float(x['value'][1]), '.3f')\n",
    "node_request_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-w1k6': '752877568.000',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '2524971008.000',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-c73w': '1441792000.000',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-rj5g': '1111490560.000'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"sum(kube_pod_container_resource_requests_memory_bytes) by (node)\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_request_ram = {}\n",
    "for x in result['data']['result']:\n",
    "    node_request_ram[x['metric']['node']] = format(float(x['value'][1]), '.3f')\n",
    "node_request_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkoutservice-784bfc794f-c68hs': '109051904.000',\n",
       " 'currencyservice-5898885559-64gws': '109051904.000',\n",
       " 'productcatalogservice-7fcf4f8cc-gk2pf': '109051904.000',\n",
       " 'redis-cart-74594bd569-825b5': '251658240.000',\n",
       " 'shippingservice-b5879cdbf-6lt2c': '109051904.000',\n",
       " 'paymentservice-6c676df669-9nqlm': '109051904.000',\n",
       " 'adservice-7cbc9bd9-s7bw7': '230686720.000',\n",
       " 'emailservice-6bd8b47657-wqq8c': '109051904.000',\n",
       " 'recommendationservice-79f5f4bbf5-tm77x': '272629760.000',\n",
       " 'cartservice-d7db78c66-hs68m': '109051904.000',\n",
       " 'frontend-764c5c755f-4zdh6': '109051904.000',\n",
       " 'loadgenerator-84cbcd768c-7r9rn': '310378496.000'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"sum(kube_pod_container_resource_requests_memory_bytes{namespace='default'}) by (pod)\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "pod_request_ram = {}\n",
    "for x in result['data']['result']:\n",
    "    pod_request_ram[x['metric']['pod']] = format(float(x['value'][1]), '.3f')\n",
    "pod_request_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'redis-cart-74594bd569-825b5': '0.080',\n",
       " 'shippingservice-b5879cdbf-6lt2c': '0.110',\n",
       " 'cartservice-d7db78c66-hs68m': '0.210',\n",
       " 'frontend-764c5c755f-4zdh6': '0.110',\n",
       " 'loadgenerator-84cbcd768c-7r9rn': '0.310',\n",
       " 'emailservice-6bd8b47657-wqq8c': '0.110',\n",
       " 'productcatalogservice-7fcf4f8cc-gk2pf': '0.110',\n",
       " 'recommendationservice-79f5f4bbf5-tm77x': '0.110',\n",
       " 'paymentservice-6c676df669-9nqlm': '0.110',\n",
       " 'checkoutservice-784bfc794f-c68hs': '0.110',\n",
       " 'currencyservice-5898885559-64gws': '0.110',\n",
       " 'adservice-7cbc9bd9-s7bw7': '0.210'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"sum(kube_pod_container_resource_requests_cpu_cores{namespace='default'}) by (pod)\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "pod_request_cpu = {}\n",
    "for x in result['data']['result']:\n",
    "    pod_request_cpu[x['metric']['pod']] = format(float(x['value'][1]), '.3f')\n",
    "pod_request_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-c73w': '6340198400',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '6340206592',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-rj5g': '6340206592',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': '6340206592'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"kube_node_status_allocatable{resource='memory'}\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_allocated_ram = {}\n",
    "for x in result['data']['result']:\n",
    "    node_allocated_ram[x['metric']['node']] = x['value'][1]\n",
    "node_allocated_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-c73w': '1.93',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': '1.93',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-rj5g': '1.93',\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': '1.93'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_request = {\"query\":\"kube_node_status_allocatable{resource='cpu'}\"}\n",
    "\n",
    "# cURL command for Node Ram Usage\n",
    "response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=app_request)\n",
    "response_status = response.status_code\n",
    "result=json.loads(response.text)\n",
    "\n",
    "node_allocated_cpu = {}\n",
    "for x in result['data']['result']:\n",
    "    node_allocated_cpu[x['metric']['node']] = x['value'][1]\n",
    "node_allocated_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-rj5g': ['frontend-764c5c755f-4zdh6',\n",
       "  'paymentservice-6c676df669-9nqlm',\n",
       "  'loadgenerator-84cbcd768c-7r9rn',\n",
       "  'cartservice-d7db78c66-hs68m'],\n",
       " 'gke-onlineboutique-default-pool-db17c72b-c73w': ['redis-cart-74594bd569-825b5',\n",
       "  'currencyservice-5898885559-64gws',\n",
       "  'checkoutservice-784bfc794f-c68hs',\n",
       "  'shippingservice-b5879cdbf-6lt2c',\n",
       "  'emailservice-6bd8b47657-wqq8c',\n",
       "  'productcatalogservice-7fcf4f8cc-gk2pf',\n",
       "  'recommendationservice-79f5f4bbf5-tm77x'],\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': [],\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': ['adservice-7cbc9bd9-s7bw7']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POD CPU USAGE\n",
    "deployment_pods = []\n",
    "pod_usage_cpu = {}\n",
    "initial_placement = {}\n",
    "for i in range(number_of_hosts):\n",
    "    query_pod_cpu = {\"query\":\"avg(rate(container_cpu_usage_seconds_total{kubernetes_io_hostname='\"+str(host_machines[i])+\"',pod!~'billowing.*', namespace='default'}[30m])) by (pod)\"}\n",
    "    pod_usage_cpu[host_machines[i]] = {}\n",
    "    \n",
    "    # cURL command for Pod Cpu Usage\n",
    "    response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=query_pod_cpu)\n",
    "    response_status = response.status_code\n",
    "    result=json.loads(response.text)\n",
    "    \n",
    "    initial_placement[host_machines[i]] = []\n",
    "    service_list = []\n",
    "    number_of_pods = len(result[\"data\"][\"result\"])\n",
    "    for k in range(number_of_pods):\n",
    "         service_list.append(result[\"data\"][\"result\"][k][\"metric\"][\"pod\"])\n",
    "         initial_placement[host_machines[i]].append(service_list[k])\n",
    "         pod_usage_cpu[host_machines[i]][service_list[k]] = format(float(result[\"data\"][\"result\"][k][\"value\"][1]), '.4f')\n",
    "    deployment_pods.append(service_list)\n",
    "    service_list.clear()\n",
    "initial_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gke-onlineboutique-default-pool-db17c72b-rj5g': {'cartservice-d7db78c66-hs68m': '52586496.0',\n",
       "  'frontend-764c5c755f-4zdh6': '50913280.0',\n",
       "  'paymentservice-6c676df669-9nqlm': '49212416.0',\n",
       "  'loadgenerator-84cbcd768c-7r9rn': '45481984.0'},\n",
       " 'gke-onlineboutique-default-pool-db17c72b-c73w': {'shippingservice-b5879cdbf-6lt2c': '36841472.0',\n",
       "  'emailservice-6bd8b47657-wqq8c': '48055296.0',\n",
       "  'productcatalogservice-7fcf4f8cc-gk2pf': '33063936.0',\n",
       "  'recommendationservice-79f5f4bbf5-tm77x': '49947648.0',\n",
       "  'redis-cart-74594bd569-825b5': '27337728.0',\n",
       "  'currencyservice-5898885559-64gws': '48508928.0',\n",
       "  'checkoutservice-784bfc794f-c68hs': '60228608.0'},\n",
       " 'gke-onlineboutique-default-pool-db17c72b-fsp8': {},\n",
       " 'gke-onlineboutique-default-pool-db17c72b-w1k6': {'adservice-7cbc9bd9-s7bw7': '95442944.0'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POD RAM USAGE\n",
    "pod_usage_ram = {}\n",
    "\n",
    "for i in range(number_of_hosts):\n",
    "    query_pod_ram = {\"query\":\"avg(container_memory_max_usage_bytes{instance='\"+host_machines[i]+\"', namespace='default', pod!~'billowing.*'}) by(pod)\"}\n",
    "    pod_usage_ram[host_machines[i]] = {}\n",
    "    \n",
    "    # cURL command for Pod Ram Usage\n",
    "    response = requests.request(\"GET\", url_prometheus, headers=headers_prometheus, params=query_pod_ram)\n",
    "    response_status = response.status_code\n",
    "    result=json.loads(response.text)\n",
    "    \n",
    "    number_of_pods = len(result[\"data\"][\"result\"])\n",
    "    for k in range(number_of_pods):\n",
    "        pod = result[\"data\"][\"result\"][k][\"metric\"][\"pod\"]\n",
    "        pod_usage_ram[host_machines[i]][pod] = format(float(result[\"data\"][\"result\"][k][\"value\"][1]), '.1f')\n",
    "pod_usage_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Integration from Kiali - Services and Affinities\n",
    "\n",
    "# Url of Kiali Graph\n",
    "url_kiali = \"http://\"+vm_external_ip+\":\"+str(kiali_port)+\"/kiali/api/namespaces/graph\"\n",
    "\n",
    "query_string_kiali = {\"duration\":\"30m\",\"namespaces\":namespace,\"graphType\":\"workload\"} # Graph type must be Wokload and i can change the graph duration\n",
    "\n",
    "headers_kiali = {\n",
    "    'cache-control': \"no-cache\"\n",
    "}\n",
    "\n",
    "# cURL command\n",
    "response = requests.request(\"GET\", url_kiali, headers=headers_kiali, params=query_string_kiali)\n",
    "\n",
    "response_status = response.status_code\n",
    "\n",
    "result=json.loads(response.text)\n",
    "\n",
    "# INFO NOTE: redis-cart won't appear from kiali graph. There must be internal communication between car\n",
    "#            cartservice and redis-cart so these two pods should be together and calculate as one\n",
    "# Graph Services ID\n",
    "services_id = {}\n",
    "unused_services_id = {}\n",
    "for i in range(len(result[\"elements\"][\"nodes\"])):\n",
    "    if(result[\"elements\"][\"nodes\"][i][\"data\"][\"namespace\"] == namespace):\n",
    "        if(\"app\" not in result[\"elements\"][\"nodes\"][i][\"data\"] or \"traffic\" not in result[\"elements\"][\"nodes\"][i][\"data\"]):\n",
    "            if(\"app\" in result[\"elements\"][\"nodes\"][i][\"data\"]):\n",
    "                key = result[\"elements\"][\"nodes\"][i][\"data\"][\"id\"]\n",
    "                unused_services_id[key] = result[\"elements\"][\"nodes\"][i][\"data\"][\"app\"]\n",
    "                continue\n",
    "            key = result[\"elements\"][\"nodes\"][i][\"data\"][\"id\"]\n",
    "            unused_services_id[key] = result[\"elements\"][\"nodes\"][i][\"data\"][\"service\"]\n",
    "            continue\n",
    "        key = result[\"elements\"][\"nodes\"][i][\"data\"][\"id\"]\n",
    "        services_id[key] = result[\"elements\"][\"nodes\"][i][\"data\"][\"app\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkoutservice': {'cartservice': '0.35',\n",
       "  'shippingservice': '0.35',\n",
       "  'emailservice': '0.18',\n",
       "  'paymentservice': '0.18',\n",
       "  'currencyservice': '0.42',\n",
       "  'productcatalogservice': '0.24'},\n",
       " 'recommendationservice': {'productcatalogservice': '2.18'},\n",
       " 'frontend': {'adservice': '1.50',\n",
       "  'cartservice': '2.82',\n",
       "  'checkoutservice': '0.18',\n",
       "  'recommendationservice': '2.18',\n",
       "  'shippingservice': '0.81',\n",
       "  'currencyservice': '8.46',\n",
       "  'productcatalogservice': '14.04'},\n",
       " 'loadgenerator': {'frontend': '2.66'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Graph edges - Affinities\n",
    "service_affinities = {}\n",
    "service_list = []\n",
    "for key in services_id:\n",
    "    service_list.append(services_id[key])\n",
    "service_list.append('redis-cart')\n",
    "\n",
    "total_edjes =len(result[\"elements\"][\"edges\"]) \n",
    "for i in range(total_edjes):\n",
    "    source_id=result[\"elements\"][\"edges\"][i][\"data\"][\"source\"] # Source ID\n",
    "    destination_id=result[\"elements\"][\"edges\"][i][\"data\"][\"target\"] # Destination ID\n",
    "    # Avoid traces from unused services dictionary\n",
    "    if((source_id in unused_services_id.keys()) or (destination_id in unused_services_id.keys())):\n",
    "        continue\n",
    "    \n",
    "    # Track all traces in service id\n",
    "    if((source_id in services_id.keys()) and (destination_id in services_id.keys())):\n",
    "        if(services_id[source_id] not in service_affinities.keys()):\n",
    "            service_affinities[services_id[source_id]] = {}\n",
    "        if(result[\"elements\"][\"edges\"][i][\"data\"][\"traffic\"][\"protocol\"] == \"http\"):\n",
    "            protocol = \"http\"\n",
    "        else:\n",
    "            protocol = \"grpc\"\n",
    "        service_affinities[services_id[source_id]][services_id[destination_id]] = result[\"elements\"][\"edges\"][i][\"data\"][\"traffic\"][\"rates\"][protocol]\n",
    "service_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pod_resources(resource_dict):\n",
    "    curr_dict = {}\n",
    "    for hosts in resource_dict:\n",
    "        for services in resource_dict[hosts]:\n",
    "            # Pattern: service_name-ID-SubID\n",
    "            split_string = re.split(\"-\", services)\n",
    "            if(len(split_string) == 3):\n",
    "                curr_service = split_string[0]\n",
    "            else:\n",
    "                curr_service = split_string[0] + '-'+ split_string[1]\n",
    "                \n",
    "            curr_dict[curr_service] = format(float(resource_dict[hosts][services]), '.3f')\n",
    "           \n",
    "    return curr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pod_requests(resource_dict):\n",
    "    curr_dict = {}\n",
    "    for services in resource_dict.keys():\n",
    "        # Pattern: service_name-ID-SubID\n",
    "        split_string = re.split(\"-\", services)\n",
    "        if(len(split_string) == 3):\n",
    "            curr_service = split_string[0]\n",
    "        else:\n",
    "            curr_service = split_string[0] + '-'+ split_string[1]\n",
    "                \n",
    "        curr_dict[curr_service] = format(float(resource_dict[services]), '.3f')\n",
    "\n",
    "    return curr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_service_names(current_placement):\n",
    "    initial_placement = {}\n",
    "    for key in current_placement:\n",
    "        initial_placement[key] = []\n",
    "        for index, services in enumerate(current_placement[key]):\n",
    "            # Pattern: service_name-ID-SubID\n",
    "            split_string = re.split(\"-\", services)\n",
    "            if(len(split_string) == 3):\n",
    "                curr_service = split_string[0]\n",
    "            else:\n",
    "                curr_service = split_string[0] + '-'+ split_string[1]\n",
    "            initial_placement[key].append(curr_service)\n",
    "    return initial_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction(services, affinities):\n",
    "    from collections import defaultdict\n",
    "  \n",
    "    # function for adding edge to graph\n",
    "    graph = defaultdict(list)\n",
    "    def addEdge(graph,u,v):\n",
    "        graph[u].append(v)\n",
    "\n",
    "    # definition of function\n",
    "    def generate_edges(graph):\n",
    "        edges = []\n",
    "\n",
    "        # for each node in graph\n",
    "        for node in graph:\n",
    "            # for each neighbour node of a single node\n",
    "            for neighbour in graph[node]:\n",
    "                # if edge exists then append\n",
    "                edges.append((node, neighbour))\n",
    "        return edges\n",
    "\n",
    "    # declaration of graph as dictionary\n",
    "    for source in affinities:\n",
    "        for dest in affinities[source]:\n",
    "            if(source in services and dest in services):\n",
    "                addEdge(graph,source,dest)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_available_node_resources(node_requests, node_max_values):\n",
    "    available_resources = {}\n",
    "    for host in node_requests:\n",
    "        available_resources[host] = float(node_max_values[host])- float(node_requests[host])\n",
    "    return available_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_node_resources(host_per_service, current_node_available_cpu, current_node_available_ram, current_node_usage_cpu, current_node_usage_ram):\n",
    "    for service in host_per_service:\n",
    "        curr_host = host_per_service[service]\n",
    "        current_node_available_cpu[curr_host] = float(current_node_available_cpu[curr_host]) + float(modified_request_cpu[service])\n",
    "        current_node_available_ram[curr_host] = float(current_node_available_ram[curr_host]) + float(modified_request_ram[service])\n",
    "        current_node_usage_cpu[curr_host] = float(current_node_usage_cpu[curr_host]) - float(modified_request_cpu[service])\n",
    "        current_node_usage_ram[curr_host] = float(current_node_usage_ram[curr_host]) - float(modified_request_cpu[service])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract_graph(parts, temp_graph, affinities):\n",
    "    curr_graph = copy.deepcopy(temp_graph)\n",
    "    \n",
    "    # Total Edjes\n",
    "    edje_count = 0\n",
    "    for x in temp_graph:\n",
    "        edje_count += len(temp_graph[x])\n",
    "    edje_count\n",
    "    \n",
    "    while edje_count > (parts - 1): # For Binary Partition we need 2 Vertices and 1 Edje - K partition -> K Vertices and K-1 Edjes(at least)\n",
    "        # Pick random source and destination whose affinity hasnt be processed\n",
    "        random_source = random.choice(list(curr_graph.keys()))\n",
    "        random_dest = random.choice((curr_graph[random_source]))\n",
    "        \n",
    "        while float(affinities[random_source][random_dest]) == 0.0:\n",
    "            random_source = random.choice(list(curr_graph.keys()))\n",
    "            random_dest = random.choice((curr_graph[random_source]))\n",
    "              \n",
    "        # Check if Random_Dest is also a source and update all the destination services for random_source\n",
    "        if random_dest in curr_graph:\n",
    "            for dest in curr_graph[random_dest]:\n",
    "\n",
    "                # Check if source contains the specific dest - otherwise add service and affinity\n",
    "                if dest in curr_graph[random_source]:\n",
    "                    affinities[random_source][dest] = format(float(float(affinities[random_source][dest]) + float(affinities[random_dest][dest])), '.4f')\n",
    "                    # Decrease Edjes\n",
    "                    edje_count -= 1\n",
    "                else:\n",
    "                    if dest == random_source:\n",
    "                        if len(curr_graph) != 2 or edje_count != 2:\n",
    "                            edje_count -= 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Append Service and Add Affinity\n",
    "                        curr_graph[random_source].append(dest)\n",
    "                        affinities[random_source][dest] = format(float(affinities[random_dest][dest]), '.4f')\n",
    "\n",
    "                # Remove affinity\n",
    "                affinities[random_dest].pop(dest)\n",
    "            curr_graph[random_dest].clear()\n",
    "            \n",
    "        \n",
    "        # Search if random_dest has other affinities with other sources \n",
    "        for key in curr_graph:\n",
    "            if key == random_source:\n",
    "                continue\n",
    "            else:\n",
    "                # Check if dest service is also in sources\n",
    "                if random_dest in curr_graph and key == random_dest:\n",
    "                    continue\n",
    "                else:\n",
    "                    # Dest in other affinity sources\n",
    "                    if random_dest in curr_graph[key]:\n",
    "                        # Random Source not in current source affinities -> add service and finally add affinity\n",
    "                        if random_source in curr_graph[key]:\n",
    "                            affinities[key][random_source] = format((float(affinities[key][random_source]) + float(affinities[key][random_dest])), '.4f')\n",
    "                            # Decrease Edjes\n",
    "                            edje_count -= 1\n",
    "                        else: \n",
    "                            curr_graph[key].append(random_source)\n",
    "                            affinities[key][random_source] = format(float(affinities[key][random_dest]), '.4f')\n",
    "                        affinities[key].pop(random_dest)\n",
    "                        curr_graph[key].remove(random_dest)\n",
    "                        \n",
    "        \n",
    "        # Update Source affinity - Remove Dest Service\n",
    "        if len(curr_graph) != 2 or edje_count != 2:\n",
    "            curr_graph[random_source].remove(random_dest)\n",
    "            affinities[random_source][random_dest] = '0.0' # Empty affinity - Means that source contains dest\n",
    "        \n",
    "        # Remove the Empty Dest if Exists \n",
    "        if random_dest in curr_graph:\n",
    "            # Check if random source contained other sources\n",
    "            if bool(affinities[random_dest]):\n",
    "                for key in affinities[random_dest]:\n",
    "                    if key not in affinities[random_source] and key != random_source:\n",
    "                        affinities[random_source][key] = '0.0'\n",
    "            \n",
    "            # Update Graph\n",
    "            curr_graph.pop(random_dest)\n",
    "            affinities.pop(random_dest)\n",
    "        \n",
    "        # Check for empty source\n",
    "        if not bool(curr_graph[random_source]):\n",
    "            curr_graph.pop(random_source)\n",
    "            \n",
    "        # Decrease Edjes\n",
    "        edje_count -= 1\n",
    "#         print(\"EDJE COUNT: \" + str(edje_count))\n",
    "        \n",
    "    # Update the partition\n",
    "    app_partition = {}\n",
    "    # Check for empty affinities\n",
    "    if len(curr_graph) != len(affinities):\n",
    "        host_service = ''\n",
    "        empty_host = ''\n",
    "        for key in affinities:\n",
    "            if key not in curr_graph:\n",
    "                empty_host = key\n",
    "            else:\n",
    "                host_service = key\n",
    "        \n",
    "        # Check the empty host services\n",
    "        for key in affinities[empty_host]:\n",
    "            if key not in affinities[host_service]:\n",
    "                affinities[host_service][key] = '0.0'\n",
    "        if empty_host not in affinities[host_service]:\n",
    "            affinities[host_service][empty_host] = '0.0'\n",
    "        affinities.pop(empty_host)\n",
    "            \n",
    "    curr_graph = copy.deepcopy(affinities)\n",
    "    for source in curr_graph:\n",
    "        app_partition[source] = []\n",
    "        for dest in curr_graph[source]:\n",
    "            if curr_graph[source][dest] != '0.0':\n",
    "                app_partition[dest] = []\n",
    "            else:\n",
    "                app_partition[source].append(dest) \n",
    "        \n",
    "    return app_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_packing(app_partition, current_placement, host_service_update, current_node_available_cpu, current_node_available_ram, current_node_usage_cpu, current_node_usage_ram):\n",
    "    final_placement = {}\n",
    "    # Iterate through all parts\n",
    "    for part in app_partition:\n",
    "        max_tf = 0.0\n",
    "        max_ml_ram = 0.0\n",
    "        max_ml_cpu = 0.0\n",
    "        max_host = ''\n",
    "        total_ram = 0.0\n",
    "        total_cpu = 0.0\n",
    "        \n",
    "         # Calculate Resource Demands\n",
    "        for service in app_partition[part]:\n",
    "                \n",
    "            # Calculate resources for current service\n",
    "            temp_cpu = float(modified_request_cpu[service])\n",
    "            temp_ram = float(modified_request_ram[service])\n",
    "\n",
    "            total_cpu += temp_cpu\n",
    "            total_ram += temp_ram\n",
    "            \n",
    "        # Iterate through available hosts\n",
    "        for host in host_list:\n",
    "            enough_resources = False\n",
    "            \n",
    "            if(total_cpu < float(current_node_available_cpu[host]) and total_ram < float(current_node_available_ram[host])):\n",
    "                enough_resources = True\n",
    "                \n",
    "            # Check if resource demands are enough\n",
    "            if(enough_resources):\n",
    "                temp_tf = 0.0\n",
    "                temp_ml_cpu = 0.0\n",
    "                temp_ml_ram = 0.0\n",
    "                    \n",
    "                # Calculate Traffic rates between services in current part of partition and services in current host\n",
    "                for service in app_partition[part]:\n",
    "                    for x in current_placement[host]:\n",
    "                        if service in current_placement[host] and x in current_placement[host]:\n",
    "                            continue # Same host\n",
    "                        else: # Different hosts\n",
    "                            if(service in service_affinities):\n",
    "                                if(x in service_affinities[service]):\n",
    "                                    temp_tf += float(service_affinities[service][x])\n",
    "                            elif(x in service_affinities):\n",
    "                                if(service in service_affinities[x]):\n",
    "                                    temp_tf += float(service_affinities[x][service])\n",
    "                    \n",
    "                # Calculate Most Loaded Situation - Prioritize CPU\n",
    "                temp_ml_cpu = float(current_node_usage_cpu[host]) + total_cpu\n",
    "                temp_ml_ram = float(current_node_usage_ram[host]) + total_ram\n",
    "                    \n",
    "                # Check Traffic Rates and Most-Loaded Situtations - Maximum searched\n",
    "                if (temp_tf > max_tf) or (temp_tf == max_tf and temp_ml_cpu > max_ml_cpu) or (temp_tf == max_tf and temp_ml_cpu == max_ml_cpu and temp_ml_ram > max_ml_ram):\n",
    "                    max_tf = temp_tf\n",
    "                    max_ml_cpu = temp_ml_cpu\n",
    "                    max_ml_ram = temp_ml_ram\n",
    "                    max_host = host\n",
    "       \n",
    "        # Check max_host\n",
    "        if max_host == '':\n",
    "            return {}\n",
    "        else:\n",
    "            current_node_available_cpu[max_host] = float(current_node_available_cpu[max_host]) - total_cpu\n",
    "            current_node_available_ram[max_host] = float(current_node_available_ram[max_host]) - total_ram\n",
    "            current_node_usage_cpu[max_host] = float(current_node_usage_cpu[max_host]) + total_cpu\n",
    "            current_node_usage_ram[max_host] = float(current_node_usage_ram[max_host]) + total_ram\n",
    "            \n",
    "            # Update placement\n",
    "            if max_host not in final_placement:\n",
    "                final_placement[max_host] = []\n",
    "                \n",
    "            for service in app_partition[part]:\n",
    "                host_service_update[service] = max_host    \n",
    "                final_placement[max_host].append(service)\n",
    "                             \n",
    "    return final_placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checkoutservice': {'currencyservice': '0.42',\n",
       "  'cartservice': '0.35',\n",
       "  'shippingservice': '0.35',\n",
       "  'productcatalogservice': '0.24',\n",
       "  'emailservice': '0.18',\n",
       "  'paymentservice': '0.18'},\n",
       " 'recommendationservice': {'productcatalogservice': '2.18'},\n",
       " 'frontend': {'currencyservice': '8.46',\n",
       "  'cartservice': '2.82',\n",
       "  'recommendationservice': '2.18',\n",
       "  'productcatalogservice': '14.04',\n",
       "  'adservice': '1.50',\n",
       "  'shippingservice': '0.81',\n",
       "  'checkoutservice': '0.18'},\n",
       " 'loadgenerator': {'frontend': '2.66'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_service_affinities = service_affinities.copy()\n",
    "for key in service_affinities:\n",
    "    sorted_service_affinities[key] = dict(sorted(sorted_service_affinities[key].items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_service_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frontend->productcatalogservice': 14.04,\n",
       " 'frontend->currencyservice': 8.46,\n",
       " 'frontend->cartservice': 2.82,\n",
       " 'loadgenerator->frontend': 2.66,\n",
       " 'recommendationservice->productcatalogservice': 2.18,\n",
       " 'frontend->recommendationservice': 2.18,\n",
       " 'frontend->adservice': 1.5,\n",
       " 'frontend->shippingservice': 0.81,\n",
       " 'checkoutservice->currencyservice': 0.42,\n",
       " 'checkoutservice->cartservice': 0.35,\n",
       " 'checkoutservice->shippingservice': 0.35,\n",
       " 'checkoutservice->productcatalogservice': 0.24,\n",
       " 'checkoutservice->emailservice': 0.18,\n",
       " 'checkoutservice->paymentservice': 0.18,\n",
       " 'frontend->checkoutservice': 0.18}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble all affinities in one matrix\n",
    "total_affinities = {}\n",
    "for source_key in sorted_service_affinities:\n",
    "    for destination_key in sorted_service_affinities[source_key]:\n",
    "        total_affinities[source_key+\"->\"+destination_key] = float(sorted_service_affinities[source_key][destination_key])\n",
    "total_affinities = dict(sorted(total_affinities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "total_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pod_requests(resource_dict):\n",
    "    curr_dict = {}\n",
    "    for services in resource_dict.keys():\n",
    "        # Pattern: service_name-ID-SubID\n",
    "        split_string = re.split(\"-\", services)\n",
    "        if(len(split_string) == 3):\n",
    "            curr_service = split_string[0]\n",
    "        else:\n",
    "            curr_service = split_string[0] + '-'+ split_string[1]\n",
    "                \n",
    "        curr_dict[curr_service] = format(float(resource_dict[services]), '.3f')\n",
    "\n",
    "    return curr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c8d98e7cfae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodified_pod_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_pod_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_usage_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodified_pod_ram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_pod_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_usage_ram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodified_request_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_pod_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_request_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodified_request_ram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_pod_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_request_ram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a52850cab0b9>\u001b[0m in \u001b[0;36mmodify_pod_resources\u001b[0;34m(resource_dict)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mcurr_service\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mcurr_service\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0msplit_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcurr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_service\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhosts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.3f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "modified_pod_cpu = modify_pod_resources(pod_usage_cpu)\n",
    "modified_pod_ram = modify_pod_resources(pod_usage_ram)\n",
    "modified_request_cpu = modify_pod_requests(pod_request_cpu)\n",
    "modified_request_ram = modify_pod_requests(pod_request_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Partition-Heuristic Packing\n",
    "# An Algorithm to produce a new optimized Service Placement according to traffic awareness and load situation\n",
    "# Input: Service based application, Affinities (Sorted-Total), Resource demands and VM available resources\n",
    "# Output: A new placement solution for current problemv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_node_available_cpu = {}\n",
    "current_node_available_ram = {}\n",
    "\n",
    "\n",
    "alpha = 1.0 # Initial amount of resources used\n",
    "delta = 0.1 # Function Step\n",
    "while alpha >= 0.0:   \n",
    "    # Gather Resources\n",
    "    current_node_available_cpu = calculate_available_node_resources(node_request_cpu,node_allocated_cpu)\n",
    "    current_node_available_ram = calculate_available_node_resources(node_request_ram,node_allocated_ram)\n",
    "    current_node_usage_cpu = copy.deepcopy(node_request_cpu)\n",
    "    current_node_usage_ram = copy.deepcopy(node_request_ram)\n",
    "   \n",
    "    # Adjusting the placement dictionary\n",
    "    current_placement = copy.deepcopy(initial_placement)\n",
    "    current_placement = adjust_service_names(current_placement)\n",
    "    \n",
    "\n",
    "    # print(\"---------------------------------------\")\n",
    "    # pprint.pprint(current_placement)\n",
    "    # pprint.pprint(current_node_available_cpu)\n",
    "    # pprint.pprint(current_node_available_ram)\n",
    "    # print(\"---------------------------------------\")\n",
    "    \n",
    "    # Partition Application\n",
    "    app_partition = {}\n",
    "    curr_partition = {}\n",
    "    curr_partition['1'] = service_list.copy()\n",
    "    total_parts = len(curr_partition)\n",
    "    k_partition = 2\n",
    "    \n",
    "    # Iterate until we find a suitable partition\n",
    "    while(True):\n",
    "        app_partition = copy.deepcopy(curr_partition)\n",
    "        # Gather Resource demands and Number of Services\n",
    "        for part in app_partition:\n",
    "            sum_cpu_usage = 0.0\n",
    "            sum_ram_usage = 0.0\n",
    "            check_resource_demands = True\n",
    "            check_number_of_services = True\n",
    "\n",
    "            # Check if part contains more than one service\n",
    "            if(len(app_partition[part]) <= 1):\n",
    "                check_number_of_services = False\n",
    "\n",
    "            # Check resource demands and if they exceed alpha \n",
    "            for service in app_partition[part]:\n",
    "                temp_cpu = float(modified_request_cpu[service])\n",
    "                temp_ram = float(modified_request_ram[service])\n",
    "\n",
    "                sum_cpu_usage += temp_cpu\n",
    "                sum_ram_usage += temp_ram\n",
    "            \n",
    "            if(sum_cpu_usage < (max_cpu_allocation * alpha) and \n",
    "               sum_ram_usage < (max_ram_allocation * alpha)):\n",
    "                check_resource_demands = False\n",
    "            \n",
    "            # Cannot meet Criteria - Partition Application Part\n",
    "            if(check_number_of_services and check_resource_demands):\n",
    "                contraction_repeats = len(app_partition[part])\n",
    "                temp_graph = graph_construction(app_partition[part], service_affinities)\n",
    "                min_graph = copy.deepcopy(temp_graph)\n",
    "                partitioned_graph = {}\n",
    "                min_sum = 0.0\n",
    "                temp_sum = 0.0\n",
    "                min_tf={}\n",
    "                \n",
    "                # Find part service affinities and min sum\n",
    "                part_service_traffic = {}\n",
    "                for source in temp_graph:\n",
    "                    part_service_traffic[source] = {}\n",
    "                    for dest in temp_graph[source]:\n",
    "                        part_service_traffic[source][dest] = float(service_affinities[source][dest])\n",
    "                        min_sum += float(service_affinities[source][dest])\n",
    "                \n",
    "                # Remove current part\n",
    "                curr_partition.pop(part)\n",
    "                \n",
    "                # Contraction Algorithm\n",
    "                while(contraction_repeats > 0):\n",
    "                    # Apply contraction Algorithm\n",
    "                    service_traffic = copy.deepcopy(part_service_traffic)\n",
    "                    partitioned_graph = contract_graph(k_partition, temp_graph, service_traffic)\n",
    "                    \n",
    "                    # Compare with minimum - Check if null\n",
    "                    if(bool(partitioned_graph)):\n",
    "                        for service in service_traffic:\n",
    "                            for x in service_traffic[service]:\n",
    "                                    temp_sum += float(service_traffic[service][x])\n",
    "                        \n",
    "                       \n",
    "                        # Check if another minimum Graph found\n",
    "                        if temp_sum < min_sum:\n",
    "                            min_graph = partitioned_graph\n",
    "                            min_sum = temp_sum\n",
    "                            min_tf = service_traffic\n",
    "                            \n",
    "                    # Decrease repeats\n",
    "                    temp_sum = 0.0\n",
    "                    contraction_repeats -= 1\n",
    "                    \n",
    "                # Partition the application and repeat process\n",
    "                for key in min_graph:\n",
    "                    curr_partition[str(total_parts+1)] = min_graph[key]\n",
    "                    curr_partition[str(total_parts+1)].append(key)\n",
    "                    total_parts += 1\n",
    "                \n",
    "        # Identical dictionaries - No changes happen - Break\n",
    "        if app_partition == curr_partition:\n",
    "            break\n",
    "    \n",
    "    partition_services = []\n",
    "    duplicate_parts = []\n",
    "    duplicate_service = []\n",
    "    for part in app_partition:\n",
    "        for service in app_partition[part]:\n",
    "            # Find duplicate services\n",
    "            if service in partition_services:\n",
    "                duplicate_service.append(service)\n",
    "                duplicate_parts.append(part)\n",
    "                continue\n",
    "            partition_services.append(service)\n",
    "    \n",
    "    # Fix duplicate services\n",
    "    part_counter = 0\n",
    "    for x in duplicate_parts:\n",
    "        app_partition[x].remove(duplicate_service[part_counter])\n",
    "        part_counter += 1\n",
    "        if not bool(app_partition[x]):\n",
    "            app_partition.pop(x)\n",
    "            \n",
    "    placement_solution = {}\n",
    "    host_service_update = copy.deepcopy(service_host)\n",
    "    # Function to calculate the true available resources without the current services of app\n",
    "    update_node_resources(host_service_update, current_node_available_cpu, current_node_available_ram, current_node_usage_cpu, current_node_usage_ram)\n",
    "\n",
    "    # Apply Heuristic Packing\n",
    "    placement_solution = heuristic_packing(app_partition, current_placement,\n",
    "                                            host_service_update, current_node_available_cpu, \n",
    "                                           current_node_available_ram, current_node_usage_cpu, \n",
    "                                           current_node_usage_ram)\n",
    "    \n",
    "    # Check if a placement solution was found\n",
    "    if bool(placement_solution): \n",
    "        # Find hosts changed during packing \n",
    "        hosts_packed = []\n",
    "        for service in host_service_update:\n",
    "            if service in partition_services:\n",
    "                if host_service_update[service] in hosts_packed:\n",
    "                    continue\n",
    "                else:\n",
    "                    hosts_packed.append(host_service_update[service])\n",
    "                    \n",
    "        # Add services with unused traffic (According to Most - Loaded situation)\n",
    "        successful_placement = True\n",
    "        for service in host_service_update:\n",
    "            host_found = False\n",
    "            if service in partition_services:\n",
    "                host_found = True\n",
    "                continue # Service Partitioned\n",
    "            else:\n",
    "                # Unpartitioned service - No traffic - Place it to most-loaded host\n",
    "                max_host = ''\n",
    "                for host in hosts_packed:\n",
    "                    # Check if service can be packed to this host\n",
    "                    if(float(modified_pod_cpu[service]) < float(current_node_available_cpu[host]) and float(modified_pod_ram[service]) < float(current_node_available_ram[host])):\n",
    "                        if(max_host != ''):\n",
    "                            if(float(current_node_available_cpu[host]) < float(current_node_available_cpu[max_host])):\n",
    "                                max_host = host\n",
    "                        else:\n",
    "                            max_host = host\n",
    "                \n",
    "               \n",
    "                if max_host != '':\n",
    "                    placement_solution[max_host].append(service)\n",
    "                    current_node_available_cpu[max_host] = float(current_node_available_cpu[max_host]) - float(modified_pod_cpu[service])\n",
    "                    current_node_available_ram[max_host] = float(current_node_available_ram[max_host]) - float(modified_pod_ram[service])\n",
    "                    current_node_usage_cpu[max_host] = float(current_node_usage_cpu[max_host]) + float(modified_pod_cpu[service])\n",
    "                    current_node_usage_ram[max_host] = float(current_node_usage_ram[max_host]) + float(modified_pod_ram[service])\n",
    "                    host_found = True\n",
    "                else:        \n",
    "                    # No Host found so check the remaining hosts of initial placement\n",
    "                    for host in host_list:\n",
    "                        if host in hosts_packed:\n",
    "                            continue # Host already Checked\n",
    "                        else:\n",
    "                             if(float(modified_pod_cpu[service]) < float(current_node_available_cpu[host]) and float(modified_pod_ram[service]) < float(current_node_available_cpu[host])):\n",
    "                                placement_solution[host].append(service)\n",
    "                                current_node_available_cpu[max_host] = float(current_node_available_cpu[max_host]) - float(modified_pod_cpu[service])\n",
    "                                current_node_available_ram[max_host] = float(current_node_available_ram[max_host]) - float(modified_pod_ram[service])\n",
    "                                current_node_usage_cpu[max_host] = float(current_node_usage_cpu[max_host]) + float(modified_pod_cpu[service])\n",
    "                                current_node_usage_ram[max_host] = float(current_node_usage_ram[max_host]) + float(modified_pod_ram[service])\n",
    "                                host_found = True\n",
    "                                break\n",
    "                \n",
    "                if host_found:\n",
    "                    continue\n",
    "                else:\n",
    "                    # No Host available found - Proceed to next alpha value\n",
    "                    successful_placement = False\n",
    "                    break\n",
    "        \n",
    "        if(successful_placement):\n",
    "            break # Placement Found - Exit\n",
    "        else:\n",
    "            continue # Proceed to next alpha\n",
    "                    \n",
    "    alpha -= delta\n",
    "placement_solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
